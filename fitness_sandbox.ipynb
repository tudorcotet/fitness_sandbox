{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58da8d73-d563-439d-a1e5-e7e80531a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b3616908b64a0499c36b4c1de0b6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Dataset and Sequences</b>'), HBox(children=(FileUpload(value=(), accept='.csv', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, VBox, HBox, Button, Label\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, DotProduct, WhiteKernel\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import norm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import io\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# For animations\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plots\n",
    "\n",
    "# Set up the color theme\n",
    "color_palette = ['#6A728A', '#6083B4', '#8F92C5', '#38D23D', '#F5A33C']\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['axes.edgecolor'] = '#4D4D4D'\n",
    "plt.rcParams['axes.labelcolor'] = '#4D4D4D'\n",
    "plt.rcParams['xtick.color'] = '#4D4D4D'\n",
    "plt.rcParams['ytick.color'] = '#4D4D4D'\n",
    "plt.rcParams['text.color'] = '#4D4D4D'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# List of amino acids\n",
    "amino_acids = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "# Global variable to hold the ESM2 model and tokenizer\n",
    "esm2_model = None\n",
    "esm2_tokenizer = None\n",
    "\n",
    "# Function to load ESM2 model when needed\n",
    "def load_esm2_model():\n",
    "    global esm2_model, esm2_tokenizer\n",
    "    if esm2_model is None or esm2_tokenizer is None:\n",
    "        print(\"Loading ESM2 model from Hugging Face...\")\n",
    "        esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "        esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "        esm2_model.eval()  # Disable dropout for deterministic results\n",
    "        print(\"ESM2 model loaded.\")\n",
    "\n",
    "# Function to encode sequences using ESM2\n",
    "def esm2_encode_sequences(sequences):\n",
    "    \"\"\"\n",
    "    Encode sequences using ESM2 embeddings from Hugging Face.\n",
    "    \"\"\"\n",
    "    load_esm2_model()\n",
    "    encoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        # Prepare sequence for the model\n",
    "        inputs = esm2_tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = esm2_model(**inputs)\n",
    "        # Get the hidden states from the last layer\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # Exclude special tokens ([CLS], [SEP]) and average embeddings\n",
    "        sequence_representation = last_hidden_states[:, 1:-1, :].mean(dim=1).squeeze().numpy()\n",
    "        encoded_sequences.append(sequence_representation)\n",
    "    return np.array(encoded_sequences)\n",
    "\n",
    "# Function to generate random sequences\n",
    "def generate_sequences(num_sequences, sequence_length):\n",
    "    sequences = [''.join(np.random.choice(amino_acids, sequence_length)) for _ in range(num_sequences)]\n",
    "    return sequences\n",
    "\n",
    "# Function to compute fitness (simulate fitness landscape)\n",
    "def compute_fitness(sequences, target_sequence):\n",
    "    \"\"\"\n",
    "    Compute fitness based on similarity to target sequence.\n",
    "    Fitness is the number of matching amino acids.\n",
    "    \"\"\"\n",
    "    fitness = []\n",
    "    for seq in sequences:\n",
    "        match_score = sum(a == b for a, b in zip(seq, target_sequence))\n",
    "        fitness.append(match_score)\n",
    "    return np.array(fitness)\n",
    "\n",
    "# Function to one-hot encode sequences\n",
    "def one_hot_encode_sequences(sequences):\n",
    "    aa_to_int = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "    encoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        seq_int = [aa_to_int[aa] for aa in seq]\n",
    "        seq_one_hot = np.eye(len(amino_acids))[seq_int]\n",
    "        seq_one_hot_flat = seq_one_hot.flatten()\n",
    "        encoded_sequences.append(seq_one_hot_flat)\n",
    "    return np.array(encoded_sequences)\n",
    "\n",
    "# Function to encode sequences using AAIndex (simulated)\n",
    "def aaindex_encode_sequences(sequences):\n",
    "    np.random.seed(42)\n",
    "    aa_properties = {aa: np.random.rand(5) for aa in amino_acids}  # 5 properties per amino acid\n",
    "    encoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        seq_encoded = np.concatenate([aa_properties[aa] for aa in seq])\n",
    "        encoded_sequences.append(seq_encoded)\n",
    "    return np.array(encoded_sequences)\n",
    "\n",
    "# VAE model class\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, x.shape[1]))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Function to train VAE\n",
    "def train_vae(sequences, encoding_dim=10, epochs=20, batch_size=32):\n",
    "    encoded_sequences = one_hot_encode_sequences(sequences)\n",
    "    input_dim = encoded_sequences.shape[1]\n",
    "    vae = VAE(input_dim, latent_dim=encoding_dim)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    dataset = torch.FloatTensor(encoded_sequences)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    vae.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(data)\n",
    "            loss = vae_loss_function(recon_batch, data, mu, logvar, criterion)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "    return vae\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar, criterion):\n",
    "    MSE = criterion(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Function to encode sequences using trained VAE\n",
    "def vae_encode_sequences(sequences, vae_model):\n",
    "    vae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_sequences = one_hot_encode_sequences(sequences)\n",
    "        data = torch.FloatTensor(encoded_sequences)\n",
    "        mu, logvar = vae_model.encode(data)\n",
    "        z = vae_model.reparameterize(mu, logvar)\n",
    "        return z.numpy()\n",
    "\n",
    "# Function to select surrogate model\n",
    "def select_surrogate_model(model_name, kernel_name='RBF'):\n",
    "    if model_name == 'Gaussian Process':\n",
    "        if kernel_name == 'RBF':\n",
    "            kernel = RBF()\n",
    "        elif kernel_name == 'Matern':\n",
    "            kernel = Matern()\n",
    "        elif kernel_name == 'RationalQuadratic':\n",
    "            kernel = RationalQuadratic()\n",
    "        elif kernel_name == 'DotProduct':\n",
    "            kernel = DotProduct()\n",
    "        else:\n",
    "            kernel = RBF()\n",
    "        kernel += WhiteKernel()\n",
    "        model = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)\n",
    "    elif model_name == 'Random Forest':\n",
    "        model = RandomForestRegressor(n_estimators=100)\n",
    "    elif model_name == 'Extra Trees':\n",
    "        model = ExtraTreesRegressor(n_estimators=100)\n",
    "    elif model_name == 'Neural Network':\n",
    "        model = MLPRegressor(hidden_layer_sizes=(100,100), max_iter=1000)\n",
    "    elif model_name == 'Deep Neural Network':\n",
    "        model = MLPRegressor(hidden_layer_sizes=(200,200,200), max_iter=1000)\n",
    "    elif model_name == '1D CNN with MC Dropout':\n",
    "        model = CNNRegressor(input_dim=input_dim, dropout_rate=0.1)\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = xgb.XGBRegressor(n_estimators=100)\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = lgb.LGBMRegressor(n_estimators=100)\n",
    "    elif model_name == 'SVR':\n",
    "        model = SVR()\n",
    "    elif model_name == 'Bayesian Ridge':\n",
    "        model = BayesianRidge()\n",
    "    elif model_name == 'ARD Regression':\n",
    "        model = ARDRegression()\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        model = GradientBoostingRegressor(n_estimators=100)\n",
    "    else:\n",
    "        model = GaussianProcessRegressor()\n",
    "    return model\n",
    "\n",
    "# Function to estimate uncertainty\n",
    "def estimate_uncertainty(model, X, model_name):\n",
    "    \"\"\"\n",
    "    Estimates the mean and uncertainty (standard deviation) of the model's predictions.\n",
    "    \"\"\"\n",
    "    if model_name == 'Gaussian Process':\n",
    "        mu, sigma = model.predict(X, return_std=True)\n",
    "    elif model_name == 'Random Forest' or model_name == 'Extra Trees':\n",
    "        # Get predictions from all trees\n",
    "        all_preds = np.array([tree.predict(X) for tree in model.estimators_])\n",
    "        mu = np.mean(all_preds, axis=0)\n",
    "        sigma = np.std(all_preds, axis=0)\n",
    "    elif model_name in ['Neural Network', 'Deep Neural Network']:\n",
    "        # Use Monte Carlo Dropout for uncertainty estimation\n",
    "        T = 10  # Number of forward passes\n",
    "        model.set_params(alpha=0.0001)  # To simulate dropout\n",
    "        preds = []\n",
    "        for _ in range(T):\n",
    "            preds.append(model.predict(X))\n",
    "        mu = np.mean(preds, axis=0)\n",
    "        sigma = np.std(preds, axis=0)\n",
    "    elif model_name == '1D CNN with MC Dropout':\n",
    "        # MC Dropout in PyTorch model\n",
    "        model.train()  # Enable dropout\n",
    "        T = 10\n",
    "        preds = []\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        for _ in range(T):\n",
    "            preds.append(model(X_tensor).detach().numpy().squeeze())\n",
    "        mu = np.mean(preds, axis=0)\n",
    "        sigma = np.std(preds, axis=0)\n",
    "    elif model_name in ['XGBoost', 'LightGBM', 'Gradient Boosting']:\n",
    "        # Approximate uncertainty using predictions from multiple models\n",
    "        # For simplicity, set sigma to a small constant value\n",
    "        mu = model.predict(X)\n",
    "        sigma = np.full_like(mu, 1e-6)\n",
    "    elif model_name in ['Bayesian Ridge', 'ARD Regression']:\n",
    "        mu, sigma = model.predict(X, return_std=True)\n",
    "    elif model_name == 'SVR':\n",
    "        mu = model.predict(X)\n",
    "        sigma = np.full_like(mu, 1e-6)\n",
    "    else:\n",
    "        mu = model.predict(X)\n",
    "        sigma = np.full_like(mu, 1e-6)\n",
    "    return mu, sigma\n",
    "\n",
    "# Expected Improvement\n",
    "def expected_improvement(mu, sigma, Y_best, xi=0.01):\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - Y_best - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    return ei\n",
    "\n",
    "# Upper Confidence Bound\n",
    "def upper_confidence_bound(mu, sigma, kappa=2.576):\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "# Probability of Improvement\n",
    "def probability_of_improvement(mu, sigma, Y_best, xi=0.01):\n",
    "    with np.errstate(divide='warn'):\n",
    "        Z = (mu - Y_best - xi) / sigma\n",
    "        pi = norm.cdf(Z)\n",
    "    return pi\n",
    "\n",
    "# Thompson Sampling\n",
    "def thompson_sampling(mu, sigma):\n",
    "    return np.random.normal(mu, sigma)\n",
    "\n",
    "# Greedy Acquisition\n",
    "def greedy_acquisition(mu):\n",
    "    return mu\n",
    "\n",
    "# eUCB Acquisition Function\n",
    "def calculate_eUCB(mu, sigma):\n",
    "    \"\"\"\n",
    "    eUCB (Exploration-weighted UCB) acquisition function.\n",
    "    Balances exploration and exploitation by incorporating uncertainty.\n",
    "    \"\"\"\n",
    "    eucb = mu + 2 * sigma\n",
    "    return eucb\n",
    "\n",
    "# Function to optimize sequences using various acquisition functions\n",
    "def optimize_sequences(sequences, fitness, encoding_method, surrogate_model_name,\n",
    "                       acquisition_function_name, kernel_name='RBF', num_mutations = 5,\n",
    "                       iterations=10, batch_size=3, target_sequence=None,\n",
    "                       user_sequences=None, user_fitness=None, sampling_strategy='Random',\n",
    "                       train_vae_flag=False, kappa=2.576, xi=0.01, exploration_type='Fixed'):\n",
    "    \"\"\"\n",
    "    Optimize sequences using the selected acquisition function.\n",
    "    \"\"\"\n",
    "    # Initialize observed data\n",
    "    if user_sequences is not None and user_fitness is not None:\n",
    "        observed_sequences = user_sequences\n",
    "        observed_fitness = user_fitness\n",
    "    else:\n",
    "        observed_sequences = sequences.copy()\n",
    "        observed_fitness = fitness.copy()\n",
    "\n",
    "    # Encoding method\n",
    "    if encoding_method == 'One-Hot':\n",
    "        encode_sequences = one_hot_encode_sequences\n",
    "    elif encoding_method == 'AAIndex':\n",
    "        encode_sequences = aaindex_encode_sequences\n",
    "    elif encoding_method == 'ESM2':\n",
    "        encode_sequences = esm2_encode_sequences\n",
    "    elif encoding_method == 'VAE':\n",
    "        # Train VAE on observed sequences\n",
    "        if train_vae_flag:\n",
    "            print(\"Training VAE...\")\n",
    "            global vae_model\n",
    "            vae_model = train_vae(observed_sequences, encoding_dim=latent_dim_widget.value)\n",
    "        encode_sequences = lambda seqs: vae_encode_sequences(seqs, vae_model)\n",
    "    else:\n",
    "        encode_sequences = one_hot_encode_sequences\n",
    "\n",
    "    # Initialize cumulative data for plotting\n",
    "    cumulative_embeddings = []\n",
    "    cumulative_fitness = []\n",
    "    cumulative_iterations = []\n",
    "\n",
    "    # Encode initial observed sequences\n",
    "    X_observed = encode_sequences(observed_sequences)\n",
    "    Y_observed = observed_fitness\n",
    "\n",
    "    # Fit PCA on initial data\n",
    "    try:\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(X_observed)\n",
    "    except:\n",
    "        reduced_embeddings = X_observed[:, :2]\n",
    "\n",
    "    # Initialize cumulative data\n",
    "    cumulative_embeddings.extend(reduced_embeddings)\n",
    "    cumulative_fitness.extend(Y_observed)\n",
    "    cumulative_iterations.extend([0] * len(Y_observed))\n",
    "\n",
    "    # Prepare for animation\n",
    "    embedding_history = []\n",
    "    fitness_history = []\n",
    "    iteration_history = []\n",
    "\n",
    "    # Main optimization loop\n",
    "    max_fitness_over_iterations = []\n",
    "    max_fitness_over_iterations.append(np.max(observed_fitness))\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Train surrogate model\n",
    "        regressor = select_surrogate_model(surrogate_model_name, kernel_name)\n",
    "        regressor_model_name = surrogate_model_name\n",
    "        if surrogate_model_name == '1D CNN with MC Dropout':\n",
    "            # Custom training for CNN in PyTorch\n",
    "            regressor = train_cnn_regressor(regressor, X_observed, Y_observed)\n",
    "        else:\n",
    "            regressor.fit(X_observed, Y_observed)\n",
    "\n",
    "        # Generate candidate sequences   \n",
    "        if user_sequences is None or user_fitness is None:\n",
    "            candidate_sequences = generate_candidate_sequences(observed_sequences, 10, num_mutations)\n",
    "            candidate_sequences = list(set(candidate_sequences) - set(observed_sequences))\n",
    "        else:\n",
    "            candidate_sequences = list(set(sequences) - set(observed_sequences))\n",
    "        \n",
    "        # Encode candidate sequences\n",
    "        X_candidates = encode_sequences(candidate_sequences)\n",
    "\n",
    "        # Predict fitness and uncertainty for candidates\n",
    "        mu, sigma = estimate_uncertainty(regressor, X_candidates, surrogate_model_name)\n",
    "\n",
    "        # Adjust explore-exploit parameters\n",
    "        if exploration_type == 'Adaptive':\n",
    "            xi = xi / (iteration + 1)\n",
    "            kappa = kappa / (iteration + 1)\n",
    "\n",
    "        # Calculate acquisition scores\n",
    "        if acquisition_function_name == 'Expected Improvement':\n",
    "            Y_best = np.max(Y_observed)\n",
    "            acquisition_values = expected_improvement(mu, sigma, Y_best, xi=xi)\n",
    "        elif acquisition_function_name == 'Probability of Improvement':\n",
    "            Y_best = np.max(Y_observed)\n",
    "            acquisition_values = probability_of_improvement(mu, sigma, Y_best, xi=xi)\n",
    "        elif acquisition_function_name == 'Upper Confidence Bound':\n",
    "            acquisition_values = upper_confidence_bound(mu, sigma, kappa=kappa)\n",
    "        elif acquisition_function_name == 'Thompson Sampling':\n",
    "            acquisition_values = thompson_sampling(mu, sigma)\n",
    "        elif acquisition_function_name == 'Greedy':\n",
    "            acquisition_values = greedy_acquisition(mu)\n",
    "        elif acquisition_function_name == 'eUCB':\n",
    "            acquisition_values = calculate_eUCB(mu, sigma)\n",
    "        else:\n",
    "            acquisition_values = expected_improvement(mu, sigma, np.max(Y_observed), xi=xi)\n",
    "\n",
    "        # Select sequences with highest acquisition scores\n",
    "        idx_top = np.argsort(-acquisition_values)[:batch_size]\n",
    "        sequences_to_evaluate = [candidate_sequences[i] for i in idx_top]\n",
    "\n",
    "        # Evaluate fitness of selected sequences\n",
    "        if target_sequence is not None:\n",
    "            fitness_evaluated = compute_fitness(sequences_to_evaluate, target_sequence)\n",
    "        elif user_sequences is not None and user_fitness is not None:\n",
    "            # Here, we match with the ground-truth data\n",
    "            fitness_evaluated = np.array([fitness[i] for i in idx_top])\n",
    "        else:\n",
    "            fitness_evaluated = np.array([fitness[i] for i in idx_top])\n",
    "\n",
    "\n",
    "        # Add selected sequences to observed data\n",
    "        observed_sequences.extend(sequences_to_evaluate)\n",
    "        observed_fitness = np.concatenate([observed_fitness, fitness_evaluated])\n",
    "\n",
    "        # Update embeddings and apply PCA transformation\n",
    "        X_new = encode_sequences(sequences_to_evaluate)\n",
    "        reduced_new_embeddings = pca.transform(X_new)\n",
    "\n",
    "        # Update cumulative data\n",
    "        cumulative_embeddings.extend(reduced_new_embeddings)\n",
    "        cumulative_fitness.extend(fitness_evaluated)\n",
    "        cumulative_iterations.extend([iteration + 1] * len(sequences_to_evaluate))\n",
    "\n",
    "        # Update X_observed and Y_observed for next iteration\n",
    "        X_observed = np.vstack((X_observed, X_new))\n",
    "        Y_observed = observed_fitness\n",
    "\n",
    "        # Update max fitness\n",
    "        max_fitness_over_iterations.append(np.max(observed_fitness))\n",
    "\n",
    "        # Append data for animation\n",
    "        embedding_history.append(np.array(cumulative_embeddings.copy()))\n",
    "        fitness_history.append(np.array(cumulative_fitness.copy()))\n",
    "        iteration_history.append(np.array(cumulative_iterations.copy()))\n",
    "\n",
    "    # Create animation of optimization progress\n",
    "    create_sequence_animation(embedding_history, fitness_history, iteration_history, cumulative_iterations)\n",
    "\n",
    "    # Plot starting seeds vs acquired sequences\n",
    "    plot_seed_vs_acquired(cumulative_embeddings, cumulative_fitness, cumulative_iterations)\n",
    "\n",
    "    # Correlation between true and predicted fitness with uncertainty\n",
    "    plot_fitness_correlation(observed_fitness, observed_sequences, regressor, encode_sequences, surrogate_model_name)\n",
    "\n",
    "    # Visualize Fitness Landscape if possible\n",
    "    if input_dim <= 2:\n",
    "        visualize_fitness_landscape(regressor, encode_sequences)\n",
    "\n",
    "    return observed_sequences, observed_fitness, max_fitness_over_iterations\n",
    "\n",
    "# Function to create sequence animation\n",
    "def create_sequence_animation(embedding_history, fitness_history, iteration_history, cumulative_iterations):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Create colorbar outside the animate function\n",
    "    norm = plt.Normalize(min(cumulative_iterations), max(cumulative_iterations))\n",
    "    mappable = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "    mappable.set_array([])  # Required for ScalarMappable\n",
    "    cbar = fig.colorbar(mappable, ax=ax, pad=0.1)\n",
    "    cbar.set_label('Iteration Acquired')\n",
    "\n",
    "    def animate(i):\n",
    "        ax.clear()\n",
    "        ax.set_title(f'Sequence Optimization Progress - Iteration {i+1}')\n",
    "        ax.set_xlabel('Embedding Dimension 1')\n",
    "        ax.set_ylabel('Embedding Dimension 2')\n",
    "        ax.set_zlabel('Fitness')\n",
    "        embeddings = embedding_history[i]\n",
    "        fitness = fitness_history[i]\n",
    "        iterations = iteration_history[i]\n",
    "        sc = ax.scatter(embeddings[:,0], embeddings[:,1], fitness, c=iterations, cmap='viridis', edgecolor='k', s=50)\n",
    "        ax.set_xlim(np.min(embeddings[:,0])-1, np.max(embeddings[:,0])+1)\n",
    "        ax.set_ylim(np.min(embeddings[:,1])-1, np.max(embeddings[:,1])+1)\n",
    "        ax.set_zlim(np.min(fitness)-1, np.max(fitness)+1)\n",
    "        return sc,\n",
    "\n",
    "    ani = FuncAnimation(fig, animate, frames=len(embedding_history), interval=1000, blit=False)\n",
    "    plt.close(fig)\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "# Function to plot starting seeds vs acquired sequences\n",
    "def plot_seed_vs_acquired(embeddings, fitness, iterations):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(np.array(embeddings)[:,0], np.array(embeddings)[:,1], fitness, c=iterations, cmap='viridis', edgecolor='k', s=50)\n",
    "    cbar = plt.colorbar(sc, ax=ax, pad=0.1)\n",
    "    cbar.set_label('Iteration Acquired')\n",
    "    ax.set_title('Sequences in Embedding Space with Fitness')\n",
    "    ax.set_xlabel('Embedding Dimension 1')\n",
    "    ax.set_ylabel('Embedding Dimension 2')\n",
    "    ax.set_zlabel('Fitness')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot correlation between true and predicted fitness with uncertainty\n",
    "def plot_fitness_correlation(true_fitness, observed_sequences, regressor, encode_sequences, model_name):\n",
    "    X_observed = encode_sequences(observed_sequences)\n",
    "    mu, sigma = estimate_uncertainty(regressor, X_observed, model_name)\n",
    "    # Ensure lengths match\n",
    "    min_length = min(len(true_fitness), len(mu))\n",
    "    true_fitness = true_fitness[:min_length]\n",
    "    mu = mu[:min_length]\n",
    "    sigma = sigma[:min_length]\n",
    "    # Compute R^2 score\n",
    "    r2 = r2_score(true_fitness, mu)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(true_fitness, mu, yerr=sigma, fmt='o', ecolor='#F5A33C', color='#6083B4', alpha=0.7, capsize=3)\n",
    "    plt.plot([min(true_fitness), max(true_fitness)], [min(true_fitness), max(true_fitness)], 'k--', lw=2)\n",
    "    plt.xlabel('True Fitness')\n",
    "    plt.ylabel('Predicted Fitness')\n",
    "    plt.title(f'True vs Predicted Fitness with Uncertainty\\n$R^2$ = {r2:.2f}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize fitness landscape (only for low-dimensional cases)\n",
    "def visualize_fitness_landscape(regressor, encode_sequences):\n",
    "    if input_dim > 2:\n",
    "        print(\"Fitness landscape visualization not available for high-dimensional input.\")\n",
    "        return\n",
    "    # Generate grid\n",
    "    x = np.linspace(0, 1, 50)\n",
    "    y = np.linspace(0, 1, 50)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mu, _ = estimate_uncertainty(regressor, grid, surrogate_model_widget.value)\n",
    "    mu = mu.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.contourf(xx, yy, mu, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted Fitness')\n",
    "    plt.title('Fitness Landscape')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# Additional functions for the optimization process\n",
    "def mutate_sequence(seq, num_mutations=1):\n",
    "    seq_list = list(seq)\n",
    "    for _ in range(num_mutations):\n",
    "        idx = np.random.randint(len(seq_list))\n",
    "        aa = np.random.choice(amino_acids)\n",
    "        seq_list[idx] = aa\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "def generate_candidate_sequences(sequences, num_candidates_per_seq, num_mutations=1):\n",
    "    candidate_sequences = []\n",
    "    for seq in sequences:\n",
    "        for _ in range(num_candidates_per_seq):\n",
    "            mutated_seq = mutate_sequence(seq, num_mutations)\n",
    "            candidate_sequences.append(mutated_seq)\n",
    "    return candidate_sequences\n",
    "\n",
    "# Function to train CNN Regressor\n",
    "def train_cnn_regressor(model, X_train, y_train):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "# Define CNN Regressor with MC Dropout\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.1):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear((input_dim - 4) * 32, 100)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Widgets for parameters\n",
    "style = {'description_width': 'initial'}\n",
    "layout = Layout(width='70%')\n",
    "\n",
    "def create_bold_label(text):\n",
    "    return widgets.HTML(value=f\"<b>{text}</b>\")\n",
    "\n",
    "# Section 1: Dataset and Sequences\n",
    "dataset_label = create_bold_label(\"Dataset and Sequences\")\n",
    "upload_button = widgets.FileUpload(\n",
    "    accept='.csv',\n",
    "    multiple=False,\n",
    "    description='Upload CSV',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Upload your dataset as a CSV file with \"sequence\" and \"fitness\" columns.'\n",
    ")\n",
    "upload_expl = Label(\"Upload your CSV file with 'sequence' and 'fitness' columns.\")\n",
    "\n",
    "num_sequences_widget = widgets.IntSlider(\n",
    "    min=1, max=100, step=1, value=10,\n",
    "    description='Number of Initial Sequences:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Set the number of initial sequences from the user-provided dataset.'\n",
    ")\n",
    "\n",
    "sequence_length_widget = widgets.IntSlider(\n",
    "    min=5, max=50, step=1, value=10,\n",
    "    description='Sequence Length:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Set the length of the protein sequences.'\n",
    ")\n",
    "\n",
    "starting_sequences_widget = widgets.Dropdown(\n",
    "    options=['Random', 'Low Fitness Similar', 'Diverse Across Fitness', 'User-Provided'],\n",
    "    value='User-Provided',\n",
    "    description='Starting Sequences:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Choose the strategy for starting sequences.'\n",
    ")\n",
    "\n",
    "starting_sequences_expl = Label(\"Choose the strategy for starting sequences.\")\n",
    "\n",
    "# Section 2: Encoding and Models\n",
    "encoding_label = create_bold_label(\"Encoding and Models\")\n",
    "encoding_method_widget = widgets.Dropdown(\n",
    "    options=['One-Hot', 'AAIndex', 'ESM2', 'VAE'],\n",
    "    value='One-Hot',\n",
    "    description='Encoding Method:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Select the sequence encoding method.'\n",
    ")\n",
    "encoding_expl = Label(\"Select the sequence encoding method.\")\n",
    "\n",
    "latent_dim_widget = widgets.IntSlider(\n",
    "    min=2, max=20, step=1, value=10,\n",
    "    description='VAE Latent Dimension:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Set the latent dimension for VAE.'\n",
    ")\n",
    "latent_dim_expl = Label(\"Set the latent dimension for VAE.\")\n",
    "\n",
    "surrogate_model_widget = widgets.Dropdown(\n",
    "    options=['Gaussian Process', 'Random Forest', 'Extra Trees', 'Neural Network', 'Deep Neural Network', '1D CNN with MC Dropout', 'XGBoost', 'LightGBM', 'SVR', 'Bayesian Ridge', 'ARD Regression', 'Gradient Boosting'],\n",
    "    value='Gaussian Process',\n",
    "    description='Regressor Model:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Choose the surrogate regressor model.'\n",
    ")\n",
    "surrogate_expl = Label(\"Choose the surrogate regressor model.\")\n",
    "\n",
    "kernel_widget = widgets.Dropdown(\n",
    "    options=['RBF', 'Matern', 'RationalQuadratic', 'DotProduct'],\n",
    "    value='RBF',\n",
    "    description='Kernel Function:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Select the kernel function for Gaussian Process.'\n",
    ")\n",
    "kernel_expl = Label(\"Select the kernel function for Gaussian Process.\")\n",
    "\n",
    "# Section 3: Optimization Settings\n",
    "optimization_label = create_bold_label(\"Optimization Settings\")\n",
    "acquisition_function_widget = widgets.Dropdown(\n",
    "    options=['Expected Improvement', 'Probability of Improvement', 'Upper Confidence Bound', 'Thompson Sampling', 'Greedy', 'eUCB'],\n",
    "    value='Expected Improvement',\n",
    "    description='Acquisition Function:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Select the acquisition function for optimization.'\n",
    ")\n",
    "acquisition_expl = Label(\"Select the acquisition function for optimization.\")\n",
    "\n",
    "iterations_widget = widgets.IntSlider(\n",
    "    min=1, max=50, step=1, value=10,\n",
    "    description='Number of Iterations:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Set the number of optimization iterations.'\n",
    ")\n",
    "iterations_expl = Label(\"Set the number of optimization iterations.\")\n",
    "\n",
    "batch_size_widget = widgets.IntSlider(\n",
    "    min=1, max=10, step=1, value=3,\n",
    "    description='Batch Size:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Number of sequences selected per iteration.'\n",
    ")\n",
    "batch_size_expl = Label(\"Number of sequences selected per iteration.\")\n",
    "\n",
    "num_mutations_widget = widgets.IntSlider(\n",
    "    min=1, max=5, step=1, value=1,\n",
    "    description='Number of Mutations:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Number of mutations applied to generate new sequences.'\n",
    ")\n",
    "mutations_expl = Label(\"Number of mutations applied to generate new sequences.\")\n",
    "\n",
    "# Explore-Exploit Parameters\n",
    "exploration_label = create_bold_label(\"Explore-Exploit Parameters\")\n",
    "kappa_widget = widgets.FloatSlider(\n",
    "    min=0.0, max=10.0, step=0.1, value=2.576,\n",
    "    description='Kappa (UCB):',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Controls exploration-exploitation trade-off in UCB.'\n",
    ")\n",
    "kappa_expl = Label(\"Controls exploration-exploitation trade-off in UCB.\")\n",
    "\n",
    "xi_widget = widgets.FloatSlider(\n",
    "    min=0.0, max=1.0, step=0.01, value=0.01,\n",
    "    description='Xi (EI/PI):',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Controls exploration in EI and PI.'\n",
    ")\n",
    "xi_expl = Label(\"Controls exploration in EI and PI.\")\n",
    "\n",
    "exploration_type_widget = widgets.Dropdown(\n",
    "    options=['Fixed', 'Adaptive'],\n",
    "    value='Fixed',\n",
    "    description='Exploration Type:',\n",
    "    style=style,\n",
    "    layout=layout,\n",
    "    tooltip='Choose whether exploration parameters are fixed or adaptive.'\n",
    ")\n",
    "exploration_type_expl = Label(\"Choose whether exploration parameters are fixed or adaptive.\")\n",
    "\n",
    "# Run Button\n",
    "run_button = Button(description=\"Run Optimization\", button_style='success', layout=Layout(width='30%', height='40px'))\n",
    "\n",
    "# Output Area\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Function to handle file upload\n",
    "global uploaded_sequences, uploaded_fitness\n",
    "\n",
    "uploaded_sequences = None\n",
    "uploaded_fitness = None\n",
    "def handle_upload(change):\n",
    "    global uploaded_sequences, uploaded_fitness\n",
    "    if change.new:  # This checks if a new file has been uploaded\n",
    "        try:\n",
    "            uploaded_file = change.new[0]  # Get the first (and usually only) uploaded file\n",
    "            content = uploaded_file['content']\n",
    "            \n",
    "            # Create a StringIO object from the content\n",
    "            string_io = io.StringIO(content.decode('utf-8'))\n",
    "            \n",
    "            # Read the CSV from the StringIO object\n",
    "            df = pd.read_csv(string_io)\n",
    "            \n",
    "            if 'sequence' in df.columns and 'fitness' in df.columns:\n",
    "                uploaded_sequences = df['sequence'].tolist()\n",
    "                uploaded_fitness = df['fitness'].values\n",
    "                print(\"CSV file loaded successfully.\")\n",
    "            else:\n",
    "                print(\"CSV file must contain 'sequence' and 'fitness' columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file: {str(e)}\")\n",
    "    else:\n",
    "        print(\"No file uploaded.\")\n",
    "upload_button.observe(handle_upload, names='value')\n",
    "print(uploaded_sequences)\n",
    "\n",
    "# Function to run optimization\n",
    "def run_optimization(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        # Prepare sequences and fitness\n",
    "        if starting_sequences_widget.value == 'Random':\n",
    "            sequences = generate_sequences(num_sequences_widget.value, sequence_length_widget.value)\n",
    "        elif starting_sequences_widget.value == 'Low Fitness Similar':\n",
    "            target_sequence = generate_sequences(1, sequence_length_widget.value)[0]\n",
    "            sequences = generate_similar_sequences(target_sequence, num_sequences_widget.value, low_fitness=True)\n",
    "        elif starting_sequences_widget.value == 'Diverse Across Fitness':\n",
    "            sequences = generate_diverse_sequences(num_sequences_widget.value, sequence_length_widget.value)\n",
    "        elif starting_sequences_widget.value == 'User-Provided':\n",
    "            uploaded_file = upload_button.value[0]\n",
    "            name = uploaded_file[\"name\"]\n",
    "            with open(name, \"wb\") as fp:\n",
    "                fp.write(uploaded_file.content)\n",
    "            df = pd.read_csv(os.path.join(os.getcwd(), name))\n",
    "        \n",
    "                \n",
    "            if 'sequence' in df.columns and 'fitness' in df.columns:\n",
    "                uploaded_sequences = df['sequence'].tolist()\n",
    "                uploaded_fitness = df['fitness'].values\n",
    "            \n",
    "            if uploaded_sequences is not None and uploaded_fitness is not None:\n",
    "                sequences = uploaded_sequences\n",
    "                fitness = uploaded_fitness\n",
    "                target_sequence = None\n",
    "                indices = np.random.choice(len(uploaded_sequences), num_sequences_widget.value, replace=False)\n",
    "                user_sequences = [uploaded_sequences[i] for i in indices]\n",
    "                user_fitness = [uploaded_fitness[i] for i in indices]\n",
    "                print(f\"Max fitness found in the starting sequences {np.max(user_fitness)}\")\n",
    "                print(f\"Max fitness in the dataset {np.max(uploaded_fitness)}\")\n",
    "            else:\n",
    "                print(\"Please upload a valid CSV file.\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"Invalid starting sequence option.\")\n",
    "            return\n",
    "\n",
    "        if starting_sequences_widget.value != 'User-Provided':\n",
    "            target_sequence = generate_sequences(1, sequence_length_widget.value)[0]\n",
    "            fitness = compute_fitness(sequences, target_sequence)\n",
    "            print(f\"Target sequence (for simulation): {target_sequence}\")\n",
    "            print(f\"Fitness of target sequence: {sequence_length_widget.value}\")\n",
    "            user_sequences = None\n",
    "            user_fitness = None\n",
    "        # Set input_dim for CNN and VAE\n",
    "        global input_dim\n",
    "        if encoding_method_widget.value == 'One-Hot':\n",
    "            input_dim = len(amino_acids) * sequence_length_widget.value\n",
    "        elif encoding_method_widget.value == 'AAIndex':\n",
    "            input_dim = 5 * sequence_length_widget.value  # 5 properties per amino acid\n",
    "        elif encoding_method_widget.value == 'ESM2':\n",
    "            load_esm2_model()\n",
    "            input_dim = esm2_model.config.hidden_size\n",
    "        elif encoding_method_widget.value == 'VAE':\n",
    "            input_dim = len(amino_acids) * sequence_length_widget.value  # Input dimension for VAE\n",
    "\n",
    "        # Train VAE if selected\n",
    "        train_vae_flag = encoding_method_widget.value == 'VAE'\n",
    "\n",
    "        # Run optimization using selected acquisition function\n",
    "        observed_sequences, observed_fitness, max_fitness_over_iterations = optimize_sequences(\n",
    "            sequences, fitness, encoding_method_widget.value, surrogate_model_widget.value,\n",
    "            acquisition_function_widget.value, kernel_name=kernel_widget.value, num_mutations = num_mutations_widget.value,\n",
    "            iterations=iterations_widget.value, batch_size=batch_size_widget.value, target_sequence=target_sequence,\n",
    "            user_sequences=user_sequences, user_fitness=user_fitness, sampling_strategy=starting_sequences_widget.value,\n",
    "            train_vae_flag=train_vae_flag, kappa=kappa_widget.value, xi=xi_widget.value,\n",
    "            exploration_type=exploration_type_widget.value)\n",
    "\n",
    "        # Plotting optimization progress\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(range(len(max_fitness_over_iterations)), max_fitness_over_iterations, marker='o', color=color_palette[1])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Maximum Fitness Observed')\n",
    "        plt.title('Optimization Progress')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Show top sequences\n",
    "        top_indices = np.argsort(-observed_fitness)[:5]\n",
    "        print(\"Top sequences found:\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"Sequence: {observed_sequences[idx]}, Fitness: {observed_fitness[idx]}\")\n",
    "\n",
    "        # Visualize ground-truth fitness distribution (if simulation)\n",
    "        if target_sequence is not None:\n",
    "            sequence_space = generate_sequences(500, sequence_length_widget.value)\n",
    "            fitness_space = compute_fitness(sequence_space, target_sequence)\n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.hist(fitness_space, bins=20, color=color_palette[0], alpha=0.7)\n",
    "            plt.xlabel('Fitness')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Ground-Truth Fitness Distribution')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "# Functions for generating starting sequences\n",
    "def generate_similar_sequences(target_sequence, num_sequences, low_fitness=False):\n",
    "    sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        seq = mutate_sequence(target_sequence, num_mutations=sequence_length_widget.value//2 if low_fitness else 1)\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "def generate_diverse_sequences(num_sequences, sequence_length):\n",
    "    sequences = set()\n",
    "    while len(sequences) < num_sequences:\n",
    "        seq = ''.join(np.random.choice(amino_acids, sequence_length))\n",
    "        sequences.add(seq)\n",
    "    return list(sequences)\n",
    "\n",
    "run_button.on_click(run_optimization)\n",
    "\n",
    "# Organize UI\n",
    "ui = VBox([\n",
    "    dataset_label,\n",
    "    HBox([upload_button, upload_expl]),\n",
    "    HBox([starting_sequences_widget, starting_sequences_expl]),\n",
    "    HBox([num_sequences_widget, sequence_length_widget]),\n",
    "    encoding_label,\n",
    "    HBox([encoding_method_widget, encoding_expl]),\n",
    "    HBox([latent_dim_widget, latent_dim_expl]),\n",
    "    HBox([surrogate_model_widget, surrogate_expl]),\n",
    "    HBox([kernel_widget, kernel_expl]),\n",
    "    optimization_label,\n",
    "    HBox([acquisition_function_widget, acquisition_expl]),\n",
    "    HBox([iterations_widget, iterations_expl]),\n",
    "    HBox([batch_size_widget, batch_size_expl]),\n",
    "    HBox([num_mutations_widget, mutations_expl]),\n",
    "    exploration_label,\n",
    "    HBox([exploration_type_widget, exploration_type_expl]),\n",
    "    HBox([kappa_widget, kappa_expl]),\n",
    "    HBox([xi_widget, xi_expl]),\n",
    "    run_button,\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078c689-be54-414b-b252-294626a2e417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scientific computing",
   "language": "python",
   "name": "scicomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
